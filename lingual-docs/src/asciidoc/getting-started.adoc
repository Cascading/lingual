[id="getting-started"]
## Getting Started

The best way to learn Lingual is to download sample data and run a few queries.

### Getting Data and Setting up Lingual

After <<install,installing Lingual>>, in a new working directory download sample data:

    > wget http://data.cascading.org/employees.tgz
    > tar -xzf employees.tgz

Next download a simple shell script to register the employee data with Lingual against the `platform` Lingual
will be executing against, either `local` or `hadoop`. `local` means data is read from the local filesystem, and the
queries are run in local memory. `hadoop` means data is read from HDFS and MapReduce jobs are run on the cluster to
execute the queries.

If `local` call:

    > export LINGUAL_PLATFORM=local

Or if `hadoop`, call:

    > export LINGUAL_PLATFORM=hadoop
    > export HADOOP_HOME=/path/to/hadoop
    > hadoop fs -copyFromLocal employees employees

Then call:

    > wget http://data.cascading.org/create-employees.sh
    > chmod a+x create-employees.sh
    > ./create-employees.sh

### The create-employees.sh script

The `create-employees.sh` script simply calls `lingual catalog` to register each file as a table, and the columns and
types in each file on the platform set by `LINGUAL_PLATFORM`.

For example, to register the `employees/employees.csv` file as the `EMPLOYEES.EMPLOYEES` table,
first the *schema* `EMPLOYEES` must be created:

    lingual catalog --schema EMPLOYEES --add

The *stereotype* must be created, named `employees` (to keep things simple):

    lingual catalog --schema EMPLOYEES --stereotype employees --add \
      --columns EMP_NO,BIRTH_DATE,FIRST_NAME,LAST_NAME,GENDER,HIRE_DATE \
      --types int,date,string,string,string,string

Then the `EMPLOYEES` table must be created:

    lingual catalog --schema EMPLOYEES --table EMPLOYEES --stereotype employees \
      --add ${BASEDIR}/employees/employees.csv

Separating *stereotype* from *table* allows the columns and type definitions to be shared across tables without
having to re-register the redundant data.

Note that `.csv` file support is built in to Lingual, so there is no need to register or create that data *format*.

### Running queries:

The Lingual Shell is simply a command shell that uses the Lingual JDBC Driver to execute SQL statements against
the configured platform.

To start the shell, run:

    > lingual shell

From within the shell, execute:

    > select * from employees.titles;

The Lingual query planner detects that we are effectively only reading the file with this query, so the results begin
to display immediately.

Alternately, run:

    > select * from employees.titles where title = 'Engineer';

This will result in an actual MapReduce job being submitted, if using the Hadoop platform. You can verify this on the
JobTracker web interface.

What actually happened under the hood is that a new Cascading Flow was created by the JDBC Driver and run to select
all the `employees` records with the given `title`, which were placed, by default, into a file in the `./results/`
directory, either on the local disk or in your user directory on HDFS.

A JDBC ResultSet was then created to read the results file where the "max rows" was set to 10,000 (the default). Since
Hadoop generally has really large files, this seems like a reasonable limit. See the command line args to change.

The file in the `./results/` directory is a valid data file, but should be deleted if you want to reclaim the
space it is taking up.

To verify on Hadoop, run:

    > hadoop fs -lsr results

Resulting in something like this:

    -rw-r--r--   3 vagrant supergroup    2165628 2013-07-24 21:01 /user/vagrant/results/20130724-210146-65127B6700/part-00000
    -rw-r--r--   3 vagrant supergroup    2169890 2013-07-24 21:01 /user/vagrant/results/20130724-210146-65127B6700/part-00001

To see the contents, run:

    hadoop fs -cat results/20130724-210146-65127B6700/part-00000

A table must exist in Lingual before an `insert into select ...` statement can be called so <<catalog,Catalog>> must
be used to create a location to insert the results into.

    > lingual catalog --schema working --add
    > lingual catalog --schema working --stereotype titles -add --columns title,cnt --types string,int
    > lingual catalog --schema working --table unique_titles --stereotype titles -add working/unique-titles.csv

Now execute the query:

    > lingual shell
    > insert into "working"."unique_titles" select title, count( title ) as cnt from employees.titles group by title;

The results will be located in `working/unique-titles.csv`.

### Using different file formats:

Lingual supports a <<provider,Data Provider>> mechanism that allows for new *formats* and *protocols* to be added
on demand.

For example, to add support for a fabricated _pipe delimited format_ or `.psv` file, the built in providers can be used
to create a new file format.

By running:

    > lingual catalog --provider

you can see the currently registered providers. `text` is the built in provider for handling delimited files.

To see the properties associated with the `text` provider, run:

    > lingual catalog --provider text --show

To create a `.psv` file, execute

    > lingual catalog --schema working --format psv --add --provider text --extensions '.psv' --properties "delimiter=|"
    > lingual catalog --schema working --table unique_titles --update working/unique-titles.psv --format psv

The results will be located in `working/unique-titles.psv` and use a `|` instead of `,` as a field delimiter.
