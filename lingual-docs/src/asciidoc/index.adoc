[id="top"]
# Lingual - SQL for Cascading (on Apache Hadoop)

Lingual executes ANSI SQL queries as http://cascading.org[Cascading] applications on
http://hadoop.apache.org[Apache Hadoop] clusters.

## Introduction

Lingual was designed to support the following goals:

Immediate Data Access:: Using the <<shell,Lingual Shell>> or <<jdbc,JDBC Driver>> with existing Desktop and BI tools,
unlock data stored on an Apache Hadoop cluster.

Simplify SQL Migration:: Using the Lingual <<jdbc,JDBC Driver>> or native <<java,Cascading APIs>>, simply migrate
existing SQL code to Apache Hadoop.

Simplify System and Data Integration:: Using the Lingual <<provider,Data Provider>> functionality, data can be moved
between any two systems over Apache Hadoop using SQL.

To satisfy these goals, Lingual supports as much of the ANSI SQL spec that makes sense, which is more than any other
tool today providing "SQL" on Apache Hadoop MapReduce. Lingual is also fully compliant with the JDBC API.

And being built on Cascading, it works seamlessly with other Cascading based workloads via the native Cascading API.

## Overview

Lingual provides JDBC Drivers, a SQL command shell, and a catalog manager for publishing files (or any resource) as
schemas and tables.

If you are already familiar with Cascading, you can use the native Cascading API to run SQL based Flows in tandem with
other custom Flows in your application.

To use Lingual, there is no installation other than the optional command line utilities.

Lingual supports `SELECT` and `INSERT INTO` statements, for example

    SELECT COUNT( DISTINCT CITY ), SUM( DISTINCT AGE ) FROM SALES.EMPS

    INSERT INTO TEST.RESULTS SELECT EMPNO, NAME FROM SALES.EMPS

DDL statements like `CREATE TABLE` are supported only through the <<catalog,Lingual Catalog>> tool for defining
Schemas and Tables from the command line.

### Command Line Tools

Use the command line tools to publish files or resources as tables, or run queries from the console, against a local
cluster or remotely on Amazon ElasticMapReduce.

To install the command line client:

  * <<install,Installation>>

The Lingual command line client consists of two utilities:

  * <<catalog,Catalog>> - for publishing files as tables
  * <<shell,Shell>> - for executing SQL statements

See the <<getting-started,Getting Started>> page for an overview of how the utilities work together.

When using Apache Hadoop, refer to the notes on how to setup your Hadoop environment:

  * <<hadoop,Using Hadoop>>

### Java JDBC

Lingual provides a fully Java 1.6 JDBC compliant Driver.

  * <<jdbc-java,A Java JDBC Example>>
  * <<jdbc-r,Accessing Data from R>>
  * <<jdbc,Using the Driver>>
  * <<jdbc-squirrel,Using the Driver with Squirrel Desktop>>

### Java Cascading APIs

Lingual provides a very simple API for executing a SQL statement as a Cascading Flow.

  * <<java, A Java Example>>

### Data Providers

*Data Providers* are `jar` files with custom Cascading Taps, Schemes, a factory class, and a provider definition file
(`provider.properties`).

When registered with Lingual using the Catalog tool, the Lingual JDBC Driver and/or Lingual Shell can run queries where
data is read or written to systems the `provider` provides integration to.

A provider can be specified by either referencing the file location, or using a Maven style spec (`group:name:revision`).

Learn more about how to create a Provider:

  * <<provider,Data Providers>>

<<top>>

include::install.adoc[]

include::getting-started.adoc[]

include::catalog.adoc[]

include::shell.adoc[]

[id="jdbc"]
## Lingual JDBC
:leveloffset: 1
include::jdbc.adoc[]
include::jdbc-java.adoc[]
include::jdbc-r.adoc[]
include::jdbc-squirrel.adoc[]

<<top>>

:leveloffset: 0

include::java.adoc[]

include::provider.adoc[]

include::hadoop.adoc[]